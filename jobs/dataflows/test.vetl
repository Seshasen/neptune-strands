{"cells":[{"cell_type":"code","metadata":{},"source":"%%configure -n project.spark.compatibility\n{\n    \"number_of_workers\": 10,\n    \"session_type\": \"etl\",\n    \"glue_version\": \"5.0\",\n    \"worker_type\": \"G.1X\",\n    \"idle_timeout\": 15,\n    \"timeout\": 60,\n    \"--enable-glue-datacatalog\": \"true\",\n    \"--enable-auto-scaling\": \"true\",\n    \"--project_s3_path\": \"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev\",\n    \"--redshift_iam_role\": \"arn:aws:iam::424988161043:role/datazone_usr_role_axc8pjvt64rd74_4s2w3mcmauwfww\",\n    \"--redshift_tempdir\": \"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev/redshift-tmp/\",\n    \"--enable-lakeformation-fine-grained-access\": \"false\"\n}","execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{},"source":"%%pyspark project.spark.compatibility\nimport sys\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\n\nimport json\nimport boto3\nimport gzip\nimport io\nimport os\nimport pyarrow.parquet as pq\nimport pyarrow.fs as pafs\nimport logging\nfrom typing import Optional\nimport random\nimport builtins\nimport csv\nfrom urllib.parse import urlparse\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import SparkSession\nfrom py4j.protocol import Py4JJavaError\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql import Row","execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{},"source":"%%pyspark project.spark.compatibility\nsc = SparkContext.getOrCreate()\nspark = SparkSession.builder.getOrCreate()\n","execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{},"source":"%%pyspark project.spark.compatibility\n# Script generated for node S3DataSource\nS3DataSource_1768564305685 = spark.read.format(\"csv\") \\\n    .option(\"inferschema\", \"true\") \\\n    .option(\"multiLine\", \"true\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"recursiveFileLookup\", \"true\") \\\n    .option(\"sep\", \",\") \\\n    .load(\"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev/\")","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"maxdomeModel":{"nodes":[{"data":{"label":"Amazon S3","nodeDescription":"JSON, CSV or Parquet file stored in S3.","iconName":"amazon_s3_colored","classification":"DataSource","senseiType":"S3","dataModel":"Structured","type":"dataSourceS3","inputShape":{"total":0},"fields":[{"fieldName":"s3SourceOptions","fieldLabel":"S3 Source Options","type":"custom","customType":"s3SourceOptionsSelector","value":{"format":"csv","paths":"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev/","sep":",","escape":"","quote":"","multiLine":"true","header":"true","inferschema":"true","recursiveFileLookup":"true"},"validator":{"_def":{"innerType":{"_def":{"typeName":"ZodDiscriminatedUnion","discriminator":"format","options":[{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":{"shape":{"format":{"_def":{"value":"csv","typeName":"ZodLiteral"},"~standard":{"version":1,"vendor":"zod"}},"paths":{"_def":{"checks":[{"kind":"min","value":1,"message":"Required"}],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"sep":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"escape":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"quote":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"multiLine":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"header":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"inferschema":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"recursiveFileLookup":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}}},"keys":["format","paths","sep","escape","quote","multiLine","header","inferschema","recursiveFileLookup"]}},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],"optionsMap":[["csv",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":{"shape":{"format":{"_def":{"value":"csv","typeName":"ZodLiteral"},"~standard":{"version":1,"vendor":"zod"}},"paths":{"_def":{"checks":[{"kind":"min","value":1,"message":"Required"}],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"sep":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"escape":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"quote":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"multiLine":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"header":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"inferschema":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}},"recursiveFileLookup":{"_def":{"innerType":{"_def":{"checks":[],"typeName":"ZodString","coerce":false},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}}},"keys":["format","paths","sep","escape","quote","multiLine","header","inferschema","recursiveFileLookup"]}}],["json",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],["parquet",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],["avro",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],["orc",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],["text",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],["hudi",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],["delta",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}],["unstructured",{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null}]]},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodOptional"},"~standard":{"version":1,"vendor":"zod"}}}]},"position":{"x":-44.5,"y":402.5},"type":"MaxDomeBasicNode","id":"1768564305685","measured":{"width":225,"height":56},"selected":true}],"edges":[],"direction":"horizontal","engine":"Glue","viewport":{"x":467.1219063368765,"y":55.27664520813059,"zoom":0.8133791981321254},"interactiveSessionModel":{"additionalPythonModules":"","connections":[],"extraJars":[],"glueVersion":"5.0","extraPyFiles":[],"iamRole":"","numberOfWorkers":10,"sessionType":"etl","tags":{},"workerType":"G.1X","command":{"name":"glueetl","pythonVersion":"3"},"timeout":60,"idleTimeout":15},"name":"","version":"1.0.0","schemasMap":[],"datasamplesMap":[],"statementsMap":[["1768564305685",{"nodeId":"1768564305685","codeSnippet":"# Script generated for node S3DataSource\nS3DataSource_1768564305685 = spark.read.format(\"csv\") \\\n    .option(\"inferschema\", \"true\") \\\n    .option(\"multiLine\", \"true\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"recursiveFileLookup\", \"true\") \\\n    .option(\"sep\", \",\") \\\n    .load(\"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev/\")","executionState":"RUNNING","statement":{"Code":"import sys\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\n\n\nimport json\n\nimport boto3\n\nimport gzip\n\nimport io\n\nimport os\n\nimport pyarrow.parquet as pq\n\nimport pyarrow.fs as pafs\n\nimport logging\n\nfrom typing import Optional\n\nimport random\n\nimport builtins\n\nimport csv\n\nfrom urllib.parse import urlparse\n\nfrom awsglue.utils import getResolvedOptions\n\nfrom pyspark.sql.functions import *\n\nfrom awsglue.context import GlueContext\n\nfrom awsglue.job import Job\n\nfrom pyspark.sql import SparkSession\n\nfrom py4j.protocol import Py4JJavaError\n\nfrom pyspark.sql.types import StringType\n\nfrom pyspark.sql import Row\n\ndef normalize_folder_file(s3_path, bucket_name, key):\n\n max_size_mb=250\n\n s3_client = boto3.client('s3')\n\n is_file = False\n\n try:\n\n     s3_client.head_object(Bucket=bucket_name, Key=key)\n\n     is_file = True\n\n except:\n\n     pass\n\n if is_file:\n\n   return [s3_path]\n\n\n\n # It's a folder, list files\n\n prefix = key if key.endswith('/') else key + '/'\n\n list_of_files = []\n\n total_size_bytes = 0\n\n max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n\n\n\n paginator = s3_client.get_paginator('list_objects_v2')\n\n\n\n near_limit = False\n\n\n\n for page in paginator.paginate(Bucket = bucket_name, Prefix = prefix):\n\n   if 'Contents' in page:\n\n     for obj in page['Contents']:\n\n       if not obj['Key'].endswith('/'):\n\n         # Get the file size in bytes\n\n         file_size = obj['Size']\n\n         # Check if adding this file would exceed our size limit\n\n         if total_size_bytes + file_size > max_size_bytes:\n\n           near_limit = True\n\n           # If we already have files and adding this one would exceed the limit, stop\n\n           break\n\n\n\n         # Add file to the list and update total size\n\n         list_of_files.append(f\"s3://{bucket_name}/{obj['Key']}\")\n\n         total_size_bytes += file_size\n\n\n\n         # If we've reached or exceeded the size limit, stop\n\n         if total_size_bytes >= max_size_bytes:\n\n           break\n\n   # If we've reached or exceeded the size limit, stop paginating\n\n     if total_size_bytes >= max_size_bytes or near_limit == True:\n\n       break\n\n # If no files found, use original path\n\n return list_of_files if list_of_files else [s3_path]\n\n\n\ndef parse_s3_path(path):\n\n if path.startswith(\"s3://\"):\n\n   parsed = urlparse(path)\n\n   return parsed.netloc, parsed.path.lstrip(\"/\")\n\n raise ValueError(\"Expected S3 URI with s3:// prefix\")\n\n\n\ndef is_single_file(path):\n\n return path.split(\"/\")[-1].count(\".\") > 0\n\n\n\ndef infer_format(path):\n\n ext = path.split(\".\")[-1].lower()\n\n return {\n\n   \"json\": \"json\",\n\n   \"csv\": \"csv\",\n\n   \"txt\": \"text\",\n\n   \"parquet\": \"parquet\",\n\n   \"avro\": \"avro\",\n\n   \"gz\": path.split(\".\")[-2].lower(),\n\n }.get(ext, None)\n\n\n\ndef sample_s3_files(source_path, bucket, prefix):\n\n files = normalize_folder_file(source_path, bucket, prefix)\n\n return random.sample(files, len(files))\n\n\n\ndef safe_stream_json_array_s3(spark, bucket, key, sample_size, max_bytes, multiline):\n\n s3 = boto3.client(\"s3\")\n\n raw = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read(max_bytes)\n\n if key.endswith(\".gz\"):\n\n   raw = gzip.decompress(raw)\n\n stream = io.StringIO(raw.decode(\"utf-8\", errors=\"replace\"))\n\n\n\n buffer = \"\"\n\n depth = 0\n\n in_string = False\n\n escaped = False\n\n objects = []\n\n reading = False\n\n while True:\n\n   c = stream.read(1)\n\n   if not c:\n\n     break\n\n   if c == '\"' and not escaped:\n\n     in_string = not in_string\n\n   elif c == '\\\\' and not escaped:\n\n     escaped = True\n\n     buffer += c\n\n     continue\n\n   else:\n\n     escaped = False\n\n\n\n   if not in_string:\n\n     if c == '{':\n\n       if depth == 0:\n\n         reading = True\n\n         buffer = \"\"\n\n       depth += 1\n\n     elif c == '}':\n\n       depth -= 1\n\n       if depth == 0:\n\n         reading = False\n\n         buffer += c\n\n         try:\n\n           obj = json.loads(buffer)\n\n           objects.append(obj)\n\n           if len(objects) >= sample_size:\n\n             break\n\n         except Exception as e:\n\n           print(f\"⚠️ Failed to parse object: {e}\")\n\n         continue\n\n   if reading or (depth > 0):\n\n     buffer += c\n\n\n\n try:\n\n   return spark.createDataFrame(objects)\n\n except Exception as e:\n\n   return spark.read.option(\"multiline\", multiline).json(f\"s3://{bucket}/{key}\").limit(sample_size)\n\n\n\ndef safe_read_json(spark, path, sample_size, max_bytes, multiline):\n\n if isinstance(max_bytes, str):\n\n   max_bytes = int(max_bytes.replace(\"_\", \"\"))\n\n bucket, key = parse_s3_path(path)\n\n if is_single_file(path):\n\n     return safe_stream_json_array_s3(spark, bucket, key, sample_size, max_bytes, multiline)\n\n else:\n\n   sample_paths = sample_s3_files(path, bucket, key)\n\n   return spark.read.option(\"multiline\", str(multiline).lower()).json(sample_paths).limit(sample_size)\n\n\n\ndef safe_read_csv(spark, path, sample_size, max_bytes, multiline, delimiter, escape, quote, inferschema, header):\n\n if isinstance(max_bytes, str):\n\n   max_bytes = int(max_bytes.replace(\"_\", \"\"))\n\n bucket, key = parse_s3_path(path)\n\n if is_single_file(path):\n\n   s3 = boto3.client(\"s3\")\n\n   raw = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read(max_bytes)\n\n   if key.endswith(\".gz\"):\n\n     raw = gzip.decompress(raw)\n\n   text = raw.decode(\"utf-8\", errors=\"replace\")\n\n   reader_args = {\n\n              'delimiter': delimiter,\n\n              'quotechar': quote,\n\n              'escapechar': escape\n\n               }\n\n    # Remove any keys where the value is None\n\n   reader_args = {k: v for k, v in reader_args.items() if v is not None}\n\n   csv_reader = csv.reader(\n\n                    io.StringIO(text),\n\n                    **reader_args\n\n                    )\n\n   column_names = []\n\n   rows = []\n\n   rows_list = list(csv_reader)\n\n   if header:\n\n     column_names = rows_list[0]\n\n     data_rows = rows_list[1:sample_size+1]\n\n   else:\n\n     num_columns = len(rows_list[0]) if rows_list else 0\n\n     column_names = [f\"col_{i}\" for i in range(num_columns)]\n\n     data_rows = rows_list[:sample_size]\n\n   # There might be some inference differences with actual csv reader but this gets the job done\n\n   if inferschema:\n\n     inferred = []\n\n     for row in data_rows:\n\n       coerced = []\n\n       for val in row:\n\n         try:\n\n           coerced.append(int(val))\n\n         except ValueError:\n\n           try:\n\n             coerced.append(float(val))\n\n           except ValueError:\n\n             coerced.append(val)\n\n       inferred.append(coerced)\n\n     data_rows = inferred\n\n   return spark.createDataFrame(data_rows, column_names)\n\n else:\n\n   sample_paths = sample_s3_files(path, bucket, key)\n\n   return spark.read.option(\"header\", header).option(\"inferSchema\", inferschema).csv(sample_paths).limit(sample_size)\n\n\n\ndef safe_read_text(spark, path, sample_size, max_bytes):\n\n return spark.read.text(path).limit(sample_size)\n\n\n\ndef safe_read_parquet(spark, path, sample_size, max_bytes):\n\n bucket, key = parse_s3_path(path)\n\n\n\n if is_single_file(path):\n\n   s3 = boto3.client(\"s3\")\n\n   # Read just the first MBs\n\n   response = s3.get_object(Bucket=bucket, Key=key, Range=f\"bytes=0-{max_bytes}\")\n\n   raw = response[\"Body\"].read()\n\n   sample_paths = [path]\n\n   try:\n\n       # Attempt to read in-memory Parquet using pyarrow\n\n       s3fs = pafs.S3FileSystem(region=os.environ.get(\"AWS_REGION\", \"us-east-1\"))\n\n       file = s3fs.open_input_file(f\"{bucket}/{key}\")\n\n       parquet_file = pq.ParquetFile(file)\n\n\n\n       # Read one or a few row groups only\n\n       max_row_groups = 1\n\n       row_groups = list(range(builtins.min(max_row_groups, parquet_file.num_row_groups)))\n\n       table = parquet_file.read_row_groups(row_groups)\n\n       limited = table.slice(0, sample_size)\n\n       return spark.createDataFrame(limited.to_pandas())\n\n   except Exception as e:\n\n       return spark.read.parquet(path).limit(sample_size)\n\n else:\n\n   # Only list and load a small number of Parquet files\n\n   sample_paths = sample_s3_files(path, bucket, key)\n\n   return spark.read.parquet(*sample_paths).limit(sample_size)\n\n\n\ndef safe_read_avro(spark, path, sample_size):\n\n return spark.read.format(\"avro\").load(path).limit(sample_size)\n\n\n\ndef safe_read_s3(\n\n spark: SparkSession,\n\n path: str,\n\n format_hint: Optional[str] = None,\n\n sample_size: int = 100,\n\n max_bytes: int = 25_000_000,\n\n multiline_json: Optional[bool] = False,\n\n delimiter: Optional[str] = ',',\n\n escape: Optional[str] = None,\n\n quote: Optional[str] = None,\n\n multiline_csv: Optional[bool] = True,\n\n inferschema: Optional[bool] = True,\n\n header: Optional[bool] = True\n\n):\n\n if isinstance(max_bytes, str):\n\n   max_bytes = int(max_bytes.replace(\"_\", \"\"))\n\n\n\n format_hint = format_hint or infer_format(path)\n\n if not format_hint:\n\n   raise ValueError(\"Could not infer format from path\")\n\n\n\n if format_hint == \"json\":\n\n   return safe_read_json(spark, path, sample_size, max_bytes, multiline_json)\n\n elif format_hint == \"csv\":\n\n   return safe_read_csv(spark, path, sample_size, max_bytes, multiline_csv, delimiter, escape, quote, inferschema, header)\n\n elif format_hint == \"text\":\n\n   return safe_read_text(spark, path, sample_size, max_bytes)\n\n elif format_hint == \"parquet\":\n\n   return safe_read_parquet(spark, path, sample_size, max_bytes)\n\n elif format_hint == \"avro\":\n\n   return safe_read_avro(spark, path, sample_size)\n\n else:\n\n   raise ValueError(f\"Unsupported format: {format_hint}\")\n\n\n\ndef safe_sample_from_catalog(\n\n    spark: SparkSession,\n\n    table: str,\n\n    sample_size: int = 1000,\n\n    repartition: int = 200,\n\n    columns: Optional[List[str]] = None,\n\n    prefer_random_partition: bool = False,\n\n    use_sampling: bool = False\n\n) -> DataFrame:\n\n    \"\"\"\n\n    Safely samples a catalog table without scanning the entire dataset.\n\n    Useful for previews or memory-safe analysis on large Glue tables.\n\n\n\n    Args:\n\n       spark: SparkSession\n\n       table: Full catalog name e.g., \"glue_db.my_table\"\n\n       sample_size: Max rows to return\n\n       repartition: Number of output partitions\n\n       columns: Optional list of columns to select\n\n       prefer_random_partition: Whether to pick a random partition (if any)\n\n       use_sampling: If True, uses df.sample() instead of df.limit()\n\n\n\n   Returns:\n\n       Spark DataFrame safely sampled\n\n   \"\"\"\n\n    is_partitioned = False\n\n    try:\n\n       # Get list of partitions\n\n       partitions = spark.sql(f\"SHOW PARTITIONS {table}\").collect()\n\n       is_partitioned = True\n\n\n\n    except Exception as e:\n\n       is_partitioned = False\n\n    if not is_partitioned:\n\n       # No partitions — fallback to full-table sampling (still dangerous)\n\n       logging.warning(f\"No partitions found for table {table}. Reading full table — risky.\")\n\n       df = spark.table(table)\n\n       sample_rows = df.limit(sample_size).collect()\n\n       return spark.createDataFrame(sample_rows)\n\n    else:\n\n       selected_partition = random.choice(partitions) if prefer_random_partition else partitions[0]\n\n       partition_clause = selected_partition[0]  # e.g., \"year=2024/month=01\"\n\n       df = spark.sql(f\"SELECT * FROM {table} WHERE {partition_clause}\")\n\n\n\n    # Optional column pruning\n\n    if columns:\n\n       df = df.select(*columns)\n\n\n\n    # Optional sampling\n\n    if use_sampling:\n\n       df = df.sample(fraction=0.001, seed=42)  # adjust fraction as needed\n\n    else:\n\n       df = df.limit(sample_size)\n\n\n\n    # Optional repartitioning\n\n    if repartition:\n\n       df = df.repartition(repartition)\n\n\n\n    return df\n\n\n\nsc = SparkContext.getOrCreate()\nspark = SparkSession.builder.getOrCreate()\n\n\ndf_S3DataSource_1768564305685 = safe_read_s3(\nspark,\npath=\"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev/\",\nformat_hint=\"csv\",\nmax_bytes=\"10_000_000\",\nmultiline_json=True,\ndelimiter=\",\",\nescape=None,\nquote=None,\nmultiline_csv=True,\ninferschema=True,\nheader=True,\n)\n\nS3DataSource_1768564305685 = df_S3DataSource_1768564305685.limit(100)\nfrom pyspark.sql.types import StructType, AtomicType\n\n# Extract schema for S3DataSource_1768564305685\nschema_json_S3DataSource_1768564305685 = S3DataSource_1768564305685.schema.json()\n\n# Filter only flat (primitive) fields from S3DataSource_1768564305685's schema\nflat_fields_S3DataSource_1768564305685 = [f.name for f in S3DataSource_1768564305685.schema.fields if isinstance(f.dataType, AtomicType)][:100]\n\n# Select flat fields and limit to 100 rows\nsample_df_S3DataSource_1768564305685 = S3DataSource_1768564305685.select(*flat_fields_S3DataSource_1768564305685).limit(100)\nsample_rows_S3DataSource_1768564305685 = sample_df_S3DataSource_1768564305685.collect()\n\n# Convert to safe stringified sample preview\nsample_data_S3DataSource_1768564305685 = [\n  {field: str(row[field]) if row[field] is not None else None for field in flat_fields_S3DataSource_1768564305685}\n  for row in sample_rows_S3DataSource_1768564305685\n]\n\n# Combine schema and sample\noutput_S3DataSource_1768564305685 = json.dumps({\n  \"Schema\": json.loads(schema_json_S3DataSource_1768564305685),\n  \"DataSample\": sample_data_S3DataSource_1768564305685\n}, default=str)\n\nprint(output_S3DataSource_1768564305685)","CompletedOn":0,"Id":0,"Output":{"Data":{"TextPlain":""},"ExecutionCount":0},"Progress":0,"StartedOn":1768564807111,"State":"RUNNING"},"executorCode":["df_S3DataSource_1768564305685 = safe_read_s3(\nspark,\npath=\"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev/\",\nformat_hint=\"csv\",\nmax_bytes=\"10_000_000\",\nmultiline_json=True,\ndelimiter=\",\",\nescape=None,\nquote=None,\nmultiline_csv=True,\ninferschema=True,\nheader=True,\n)\n\nS3DataSource_1768564305685 = df_S3DataSource_1768564305685.limit(100)\nfrom pyspark.sql.types import StructType, AtomicType\n\n# Extract schema for S3DataSource_1768564305685\nschema_json_S3DataSource_1768564305685 = S3DataSource_1768564305685.schema.json()\n\n# Filter only flat (primitive) fields from S3DataSource_1768564305685's schema\nflat_fields_S3DataSource_1768564305685 = [f.name for f in S3DataSource_1768564305685.schema.fields if isinstance(f.dataType, AtomicType)][:100]\n\n# Select flat fields and limit to 100 rows\nsample_df_S3DataSource_1768564305685 = S3DataSource_1768564305685.select(*flat_fields_S3DataSource_1768564305685).limit(100)\nsample_rows_S3DataSource_1768564305685 = sample_df_S3DataSource_1768564305685.collect()\n\n# Convert to safe stringified sample preview\nsample_data_S3DataSource_1768564305685 = [\n  {field: str(row[field]) if row[field] is not None else None for field in flat_fields_S3DataSource_1768564305685}\n  for row in sample_rows_S3DataSource_1768564305685\n]\n\n# Combine schema and sample\noutput_S3DataSource_1768564305685 = json.dumps({\n  \"Schema\": json.loads(schema_json_S3DataSource_1768564305685),\n  \"DataSample\": sample_data_S3DataSource_1768564305685\n}, default=str)\n\nprint(output_S3DataSource_1768564305685)"]}]],"createSessionRequest":{"Role":"arn:aws:iam::424988161043:role/datazone_usr_role_axc8pjvt64rd74_4s2w3mcmauwfww","Command":{"Name":"glueetl","PythonVersion":"3"},"DefaultArguments":{"--enable-glue-datacatalog":"true","--enable-auto-scaling":"true","--project_s3_path":"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev","--redshift_iam_role":"arn:aws:iam::424988161043:role/datazone_usr_role_axc8pjvt64rd74_4s2w3mcmauwfww","--redshift_tempdir":"s3://amazon-sagemaker-424988161043-us-west-2-c360e35e1a5c/dzd_5d3ljv8t3n0fkg/axc8pjvt64rd74/dev/redshift-tmp/","--enable-lakeformation-fine-grained-access":"false"},"GlueVersion":"5.0","RequestOrigin":"SageMakerUnifiedStudio_VisualETLDataPreview","Timeout":60,"IdleTimeout":15,"NumberOfWorkers":10,"WorkerType":"G.1X","Tags":{"AmazonDataZoneProject":"axc8pjvt64rd74"}},"dpSettings":{"autoRestart":true,"sampleSize":100,"sampleColSize":100},"computeConnection":{"configurations":[{"classification":"GlueDefaultArgument","properties":{"--enable-lakeformation-fine-grained-access":"false"}}],"domainIdentifier":"dzd_5d3ljv8t3n0fkg","projectIdentifier":"axc8pjvt64rd74","environmentIdentifier":"4s2w3mcmauwfww","connectionIdentifier":"cyxtwesy83ejv4","name":"project.spark.compatibility","displayType":"Glue (Spark)","type":"SPARK","authorizationMode":"PROJECT","provisioningType":"MANAGED","createdAt":"2026-01-16T01:31:00.406Z","updatedAt":"2026-01-16T01:31:00.406Z","location":{"awsRegion":"us-west-2","awsAccountId":"424988161043"},"physicalEndpoints":[{"awsLocation":{"awsAccountId":"424988161043","awsRegion":"us-west-2"},"glueConnectionName":"datazone-glue-network-connection-axc8pjvt64rd74-dev"}],"sparkGlueProperties":{"glueConnection":{},"sessionConfigs":{"idle_timeout":60,"glue_version":"5.0","worker_type":"G.1X","number_of_workers":10}}},"isSaved":true,"glueJobArn":"arn:aws:glue:us-west-2:424988161043:job/test","isEncrypted":true,"dagOverrideFlags":[]}},"nbformat":4,"nbformat_minor":5}